{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTp_7Z2RAKoh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import dataset\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "\n",
        "def get_all_column_names(predicates):\n",
        "    column_names = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                column_names.add(column_name)\n",
        "    return column_names\n",
        "\n",
        "\n",
        "def get_all_table_names(tables):\n",
        "    table_names = set()\n",
        "    for query in tables:\n",
        "        for table in query:\n",
        "            table_names.add(table)\n",
        "    return table_names\n",
        "\n",
        "\n",
        "def get_all_operators(predicates):\n",
        "    operators = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                operator = predicate[1]\n",
        "                operators.add(operator)\n",
        "    return operators\n",
        "\n",
        "\n",
        "def get_all_joins(joins):\n",
        "    join_set = set()\n",
        "    for query in joins:\n",
        "        for join in query:\n",
        "            join_set.add(join)\n",
        "    return join_set\n",
        "\n",
        "\n",
        "def idx_to_onehot(idx, num_elements):\n",
        "    onehot = np.zeros(num_elements, dtype=np.float32)\n",
        "    onehot[idx] = 1.\n",
        "    return onehot\n",
        "\n",
        "\n",
        "def get_set_encoding(source_set, onehot=True):\n",
        "    num_elements = len(source_set)\n",
        "    source_list = list(source_set)\n",
        "    # Sort list to avoid non-deterministic behavior\n",
        "    source_list.sort()\n",
        "    # Build map from s to i\n",
        "    thing2idx = {s: i for i, s in enumerate(source_list)}\n",
        "    # Build array (essentially a map from idx to s)\n",
        "    idx2thing = [s for i, s in enumerate(source_list)]\n",
        "    if onehot:\n",
        "        thing2vec = {s: idx_to_onehot(i, num_elements) for i, s in enumerate(source_list)}\n",
        "        return thing2vec, idx2thing\n",
        "    return thing2idx, idx2thing\n",
        "\n",
        "\n",
        "def get_min_max_vals(predicates, column_names):\n",
        "    min_max_vals = {t: [float('inf'), float('-inf')] for t in column_names}\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                val = float(predicate[2])\n",
        "                if val < min_max_vals[column_name][0]:\n",
        "                    min_max_vals[column_name][0] = val\n",
        "                if val > min_max_vals[column_name][1]:\n",
        "                    min_max_vals[column_name][1] = val\n",
        "    return min_max_vals\n",
        "\n",
        "\n",
        "def normalize_data(val, column_name, column_min_max_vals):\n",
        "    min_val = column_min_max_vals[column_name][0]\n",
        "    max_val = column_min_max_vals[column_name][1]\n",
        "    val = float(val)\n",
        "    val_norm = 0.0\n",
        "    if max_val > min_val:\n",
        "        val_norm = (val - min_val) / (max_val - min_val)\n",
        "    return np.array(val_norm, dtype=np.float32)\n",
        "\n",
        "\n",
        "def normalize_labels(labels, min_val=None, max_val=None):\n",
        "    labels = np.array([np.log(float(l)) for l in labels])\n",
        "    if min_val is None:\n",
        "        min_val = labels.min()\n",
        "        print(\"min log(label): {}\".format(min_val))\n",
        "    if max_val is None:\n",
        "        max_val = labels.max()\n",
        "        print(\"max log(label): {}\".format(max_val))\n",
        "    labels_norm = (labels - min_val) / (max_val - min_val)\n",
        "    # Threshold labels\n",
        "    labels_norm = np.minimum(labels_norm, 1)\n",
        "    labels_norm = np.maximum(labels_norm, 0)\n",
        "    return labels_norm, min_val, max_val\n",
        "\n",
        "\n",
        "def unnormalize_labels(labels_norm, min_val, max_val):\n",
        "    labels_norm = np.array(labels_norm, dtype=np.float32)\n",
        "    labels = (labels_norm * (max_val - min_val)) + min_val\n",
        "    return np.array(np.round(np.exp(labels)), dtype=np.int64)\n",
        "\n",
        "\n",
        "def encode_samples(tables, samples, table2vec):\n",
        "    samples_enc = []\n",
        "    for i, query in enumerate(tables):\n",
        "        samples_enc.append(list())\n",
        "        for j, table in enumerate(query):\n",
        "            sample_vec = []\n",
        "            # Append table one-hot vector\n",
        "            sample_vec.append(table2vec[table])\n",
        "            # Append bit vector\n",
        "            sample_vec.append(samples[i][j])\n",
        "            sample_vec = np.hstack(sample_vec)\n",
        "            samples_enc[i].append(sample_vec)\n",
        "    return samples_enc\n",
        "\n",
        "\n",
        "def encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
        "    predicates_enc = []\n",
        "    joins_enc = []\n",
        "    for i, query in enumerate(predicates):\n",
        "        predicates_enc.append(list())\n",
        "        joins_enc.append(list())\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                # Proper predicate\n",
        "                column = predicate[0]\n",
        "                operator = predicate[1]\n",
        "                val = predicate[2]\n",
        "                norm_val = normalize_data(val, column, column_min_max_vals)\n",
        "\n",
        "                pred_vec = []\n",
        "                pred_vec.append(column2vec[column])\n",
        "                pred_vec.append(op2vec[operator])\n",
        "                pred_vec.append(norm_val)\n",
        "                pred_vec = np.hstack(pred_vec)\n",
        "            else:\n",
        "                pred_vec = np.zeros((len(column2vec) + len(op2vec) + 1))\n",
        "\n",
        "            predicates_enc[i].append(pred_vec)\n",
        "\n",
        "        for predicate in joins[i]:\n",
        "            # Join instruction\n",
        "            join_vec = join2vec[predicate]\n",
        "            joins_enc[i].append(join_vec)\n",
        "    return predicates_enc, joins_enc\n",
        "\n",
        "#Modified model with CNN and residual connection\n",
        "class ModifiedSetConv(nn.Module):\n",
        "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
        "        super(ModifiedSetConv, self).__init__()\n",
        "        self.dropout_rate = 0.2  # Adjustable dropout rate\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.sample_conv1 = nn.Conv1d(in_channels=sample_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "        self.sample_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "        self.predicate_conv1 = nn.Conv1d(in_channels=predicate_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "        self.predicate_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "        self.join_conv1 = nn.Conv1d(in_channels=join_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "        self.join_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "        # Residual connection\n",
        "        self.sample_res = nn.Conv1d(in_channels=sample_feats, out_channels=hid_units, kernel_size=1)\n",
        "        self.predicate_res = nn.Conv1d(in_channels=predicate_feats, out_channels=hid_units, kernel_size=1)\n",
        "        self.join_res = nn.Conv1d(in_channels=join_feats, out_channels=hid_units, kernel_size=1)\n",
        "\n",
        "        # Output layers\n",
        "        self.fc1 = nn.Linear(hid_units * 3, hid_units)\n",
        "        self.fc2 = nn.Linear(hid_units, 1)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
        "        # Reshape input to fit Conv1d (batch_size, channels, sequence_length)\n",
        "        samples = samples.permute(0, 2, 1)\n",
        "        predicates = predicates.permute(0, 2, 1)\n",
        "        joins = joins.permute(0, 2, 1)\n",
        "\n",
        "        # Sample path\n",
        "        x_sample_res = self.sample_res(samples)\n",
        "        x_sample = F.relu(self.sample_conv1(samples))\n",
        "        x_sample = F.relu(self.sample_conv2(x_sample) + x_sample_res)  # Adding residual connection\n",
        "        x_sample = torch.sum(x_sample, dim=2) / sample_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "        # Predicate path\n",
        "        x_predicate_res = self.predicate_res(predicates)\n",
        "        x_predicate = F.relu(self.predicate_conv1(predicates))\n",
        "        x_predicate = F.relu(self.predicate_conv2(x_predicate) + x_predicate_res)  # Adding residual connection\n",
        "        x_predicate = torch.sum(x_predicate, dim=2) / predicate_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "        # Join path\n",
        "        x_join_res = self.join_res(joins)\n",
        "        x_join = F.relu(self.join_conv1(joins))\n",
        "        x_join = F.relu(self.join_conv2(x_join) + x_join_res)  # Adding residual connection\n",
        "        x_join = torch.sum(x_join, dim=2) / join_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "        # Combine and process final outputs\n",
        "        x = torch.cat((x_sample, x_predicate, x_join), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        out = torch.sigmoid(self.fc2(x))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "#Modified model with CNN\n",
        "# class SetConv_CNN(nn.Module):\n",
        "#     def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
        "#         super(SetConv_CNN, self).__init__()\n",
        "#         self.dropout_rate = 0.2  # Adjust dropout rate as needed\n",
        "\n",
        "#         # Define convolutional layers for each path with kernel size, stride and padding appropriately selected\n",
        "#         self.sample_conv1 = nn.Conv1d(in_channels=sample_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "#         self.sample_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "#         self.predicate_conv1 = nn.Conv1d(in_channels=predicate_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "#         self.predicate_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "#         self.join_conv1 = nn.Conv1d(in_channels=join_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "#         self.join_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "#         # Define output layers\n",
        "#         self.fc1 = nn.Linear(hid_units * 3, hid_units)  # Combine features from all three paths\n",
        "#         self.fc2 = nn.Linear(hid_units, 1)\n",
        "#         self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    # def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
        "    #     # Reshape input to fit Conv1d (batch_size, channels, sequence_length)\n",
        "    #     samples = samples.permute(0, 2, 1)\n",
        "    #     predicates = predicates.permute(0, 2, 1)\n",
        "    #     joins = joins.permute(0, 2, 1)\n",
        "\n",
        "    #     # Sample path\n",
        "    #     x_sample = F.relu(self.sample_conv1(samples))\n",
        "    #     x_sample = F.relu(self.sample_conv2(x_sample))\n",
        "    #     x_sample = torch.sum(x_sample, dim=2) / sample_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "    #     # Predicate path\n",
        "    #     x_predicate = F.relu(self.predicate_conv1(predicates))\n",
        "    #     x_predicate = F.relu(self.predicate_conv2(x_predicate))\n",
        "    #     x_predicate = torch.sum(x_predicate, dim=2) / predicate_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "    #     # Join path\n",
        "    #     x_join = F.relu(self.join_conv1(joins))\n",
        "    #     x_join = F.relu(self.join_conv2(x_join))\n",
        "    #     x_join = torch.sum(x_join, dim=2) / join_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "    #     # Combine and process final outputs\n",
        "    #     x = torch.cat((x_sample, x_predicate, x_join), dim=1)\n",
        "    #     x = F.relu(self.fc1(x))\n",
        "    #     x = self.dropout(x)\n",
        "    #     out = torch.sigmoid(self.fc2(x))\n",
        "\n",
        "    #     return out\n",
        "\n",
        "\n",
        "def load_data(file_name, num_materialized_samples):\n",
        "    joins = []\n",
        "    predicates = []\n",
        "    tables = []\n",
        "    samples = []\n",
        "    label = []\n",
        "\n",
        "    # Load queries\n",
        "    with open(file_name + \".csv\", 'rU') as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
        "        for row in data_raw:\n",
        "            tables.append(row[0].split(','))\n",
        "            joins.append(row[1].split(','))\n",
        "            predicates.append(row[2].split(','))\n",
        "            if int(row[3]) < 1:\n",
        "                print(\"Queries must have non-zero cardinalities\")\n",
        "                exit(1)\n",
        "            label.append(row[3])\n",
        "    print(\"Loaded queries\")\n",
        "\n",
        "    # Load bitmaps\n",
        "    num_bytes_per_bitmap = int((num_materialized_samples + 7) >> 3)\n",
        "    with open(file_name + \".bitmaps\", 'rb') as f:\n",
        "        for i in range(len(tables)):\n",
        "            four_bytes = f.read(4)\n",
        "            if not four_bytes:\n",
        "                print(\"Error while reading 'four_bytes'\")\n",
        "                exit(1)\n",
        "            num_bitmaps_curr_query = int.from_bytes(four_bytes, byteorder='little')\n",
        "            bitmaps = np.empty((num_bitmaps_curr_query, num_bytes_per_bitmap * 8), dtype=np.uint8)\n",
        "            for j in range(num_bitmaps_curr_query):\n",
        "                # Read bitmap\n",
        "                bitmap_bytes = f.read(num_bytes_per_bitmap)\n",
        "                if not bitmap_bytes:\n",
        "                    print(\"Error while reading 'bitmap_bytes'\")\n",
        "                    exit(1)\n",
        "                bitmaps[j] = np.unpackbits(np.frombuffer(bitmap_bytes, dtype=np.uint8))\n",
        "            samples.append(bitmaps)\n",
        "    print(\"Loaded bitmaps\")\n",
        "\n",
        "    # Split predicates\n",
        "    predicates = [list(chunks(d, 3)) for d in predicates]\n",
        "\n",
        "    return joins, predicates, tables, samples, label\n",
        "\n",
        "\n",
        "\n",
        "def load_and_encode_train_data(num_queries, num_materialized_samples):\n",
        "    file_name_queries = \"/content/data/train\"\n",
        "    file_name_column_min_max_vals = \"/content/data/column_min_max_vals.csv\"\n",
        "\n",
        "    joins, predicates, tables, samples, label = load_data(file_name_queries, num_materialized_samples)\n",
        "\n",
        "    # Get column name dict\n",
        "    column_names = get_all_column_names(predicates)\n",
        "    column2vec, idx2column = get_set_encoding(column_names)\n",
        "\n",
        "    # Get table name dict\n",
        "    table_names = get_all_table_names(tables)\n",
        "    table2vec, idx2table = get_set_encoding(table_names)\n",
        "\n",
        "    # Get operator name dict\n",
        "    operators = get_all_operators(predicates)\n",
        "    op2vec, idx2op = get_set_encoding(operators)\n",
        "\n",
        "    # Get join name dict\n",
        "    join_set = get_all_joins(joins)\n",
        "    join2vec, idx2join = get_set_encoding(join_set)\n",
        "\n",
        "    # Get min and max values for each column\n",
        "    with open(file_name_column_min_max_vals, 'rU') as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
        "        column_min_max_vals = {}\n",
        "        for i, row in enumerate(data_raw):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_enc = encode_samples(tables, samples, table2vec)\n",
        "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    label_norm, min_val, max_val = normalize_labels(label)\n",
        "\n",
        "    # Split in training and validation samples\n",
        "    num_train = int(num_queries * 0.9)\n",
        "    num_test = num_queries - num_train\n",
        "\n",
        "    samples_train = samples_enc[:num_train]\n",
        "    predicates_train = predicates_enc[:num_train]\n",
        "    joins_train = joins_enc[:num_train]\n",
        "    labels_train = label_norm[:num_train]\n",
        "\n",
        "    samples_test = samples_enc[num_train:num_train + num_test]\n",
        "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
        "    joins_test = joins_enc[num_train:num_train + num_test]\n",
        "    labels_test = label_norm[num_train:num_train + num_test]\n",
        "\n",
        "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
        "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
        "\n",
        "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
        "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
        "\n",
        "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
        "    train_data = [samples_train, predicates_train, joins_train]\n",
        "    test_data = [samples_test, predicates_test, joins_test]\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
        "\n",
        "\n",
        "def make_dataset(samples, predicates, joins, labels, max_num_joins, max_num_predicates):\n",
        "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
        "\n",
        "    sample_masks = []\n",
        "    sample_tensors = []\n",
        "    for sample in samples:\n",
        "        sample_tensor = np.vstack(sample)\n",
        "        num_pad = max_num_joins + 1 - sample_tensor.shape[0]\n",
        "        sample_mask = np.ones_like(sample_tensor).mean(1, keepdims=True)\n",
        "        sample_tensor = np.pad(sample_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_mask = np.pad(sample_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_tensors.append(np.expand_dims(sample_tensor, 0))\n",
        "        sample_masks.append(np.expand_dims(sample_mask, 0))\n",
        "    sample_tensors = np.vstack(sample_tensors)\n",
        "    sample_tensors = torch.FloatTensor(sample_tensors)\n",
        "    sample_masks = np.vstack(sample_masks)\n",
        "    sample_masks = torch.FloatTensor(sample_masks)\n",
        "\n",
        "    predicate_masks = []\n",
        "    predicate_tensors = []\n",
        "    for predicate in predicates:\n",
        "        predicate_tensor = np.vstack(predicate)\n",
        "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
        "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
        "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
        "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
        "    predicate_tensors = np.vstack(predicate_tensors)\n",
        "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
        "    predicate_masks = np.vstack(predicate_masks)\n",
        "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
        "\n",
        "    join_masks = []\n",
        "    join_tensors = []\n",
        "    for join in joins:\n",
        "        join_tensor = np.vstack(join)\n",
        "        num_pad = max_num_joins - join_tensor.shape[0]\n",
        "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
        "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
        "        join_masks.append(np.expand_dims(join_mask, 0))\n",
        "    join_tensors = np.vstack(join_tensors)\n",
        "    join_tensors = torch.FloatTensor(join_tensors)\n",
        "    join_masks = np.vstack(join_masks)\n",
        "    join_masks = torch.FloatTensor(join_masks)\n",
        "\n",
        "    target_tensor = torch.FloatTensor(labels)\n",
        "\n",
        "    return dataset.TensorDataset(sample_tensors, predicate_tensors, join_tensors, target_tensor, sample_masks,\n",
        "                                 predicate_masks, join_masks)\n",
        "\n",
        "\n",
        "def get_train_datasets(num_queries, num_materialized_samples):\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(\n",
        "        num_queries, num_materialized_samples)\n",
        "    train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins,\n",
        "                                 max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for training data\")\n",
        "    test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins,\n",
        "                                max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for validation data\")\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "def unnormalize_torch(vals, min_val, max_val):\n",
        "    vals = (vals * (max_val - min_val)) + min_val\n",
        "    return torch.exp(vals)\n",
        "\n",
        "\n",
        "def qerror_loss(preds, targets, min_val, max_val):\n",
        "    qerror = []\n",
        "    preds = unnormalize_torch(preds, min_val, max_val)\n",
        "    targets = unnormalize_torch(targets, min_val, max_val)\n",
        "\n",
        "    for i in range(len(targets)):\n",
        "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
        "            qerror.append(preds[i] / targets[i])\n",
        "        else:\n",
        "            qerror.append(targets[i] / preds[i])\n",
        "    return torch.mean(torch.cat(qerror))\n",
        "\n",
        "\n",
        "def predict(model, data_loader, cuda):\n",
        "    preds = []\n",
        "    t_total = 0.\n",
        "    model.eval()\n",
        "    for batch_idx, data_batch in enumerate(data_loader):\n",
        "\n",
        "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "        if cuda:\n",
        "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
        "            targets)\n",
        "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
        "            join_masks)\n",
        "\n",
        "        t = time.time()\n",
        "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "        t_total += time.time() - t\n",
        "\n",
        "        for i in range(outputs.data.shape[0]):\n",
        "            preds.append(outputs.data[i])\n",
        "\n",
        "    return preds, t_total\n",
        "\n",
        "\n",
        "\n",
        "def print_qerror(preds_unnorm, labels_unnorm):\n",
        "    qerror = []\n",
        "    # Ensure inputs are numpy arrays for consistent handling\n",
        "    preds_unnorm = np.array(preds_unnorm, dtype=np.float32)\n",
        "    labels_unnorm = np.array(labels_unnorm, dtype=np.float32)\n",
        "\n",
        "    for i in range(len(preds_unnorm)):\n",
        "        pred = preds_unnorm[i]\n",
        "        label = labels_unnorm[i]\n",
        "        # Ensure both pred and label are scalars for division\n",
        "        if pred > label:\n",
        "            qerror.append(pred / label)\n",
        "        else:\n",
        "            qerror.append(label / pred)\n",
        "\n",
        "    qerror = np.array(qerror)  # Convert list to numpy array for statistical functions\n",
        "    print(\"Median: {}\".format(np.median(qerror)))\n",
        "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
        "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
        "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
        "    print(\"Max: {}\".format(np.max(qerror)))\n",
        "    print(\"Mean: {}\".format(np.mean(qerror)))\n",
        "\n",
        "\n",
        "def train_and_predict(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda=False):\n",
        "    # Load training and validation data\n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        num_queries, num_materialized_samples)\n",
        "\n",
        "    # Train model\n",
        "    sample_feats = len(dicts[0]) + num_materialized_samples\n",
        "    predicate_feats = len(dicts[1]) + len(dicts[2]) + 1\n",
        "    join_feats = len(dicts[3])\n",
        "\n",
        "    model = ModifiedSetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Data loaders\n",
        "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    # Train model\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_total = 0.\n",
        "\n",
        "        for batch_idx, data_batch in enumerate(train_data_loader):\n",
        "            samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "            # Run model\n",
        "            outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "            loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
        "            loss_total += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds_test, _ = predict(model, test_data_loader, cuda)\n",
        "        preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "        labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
        "\n",
        "        print(\"\\nQ-Error validation set:\")\n",
        "        print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
        "\n",
        "def main(testset='synthetic', queries=10000, epochs=10, batch=1024, hid=256):\n",
        "    train_and_predict(testset, queries, epochs, batch, hid, cuda=False)\n",
        "\n",
        "# Example call\n",
        "main(testset='scale', queries=6000, epochs=150, batch=400, hid=128)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = list(range(150))  # Assuming 150 epochs\n",
        "loss_values = [\n",
        "    395.1378261021205, 241.07403346470423, 86.52479090009417, 51.32700783865793, 36.026547704424175,\n",
        "    29.127995491027832, 25.21650423322405, 26.037455013820104, 20.065981115613663, 13.363799231392997,\n",
        "    11.164984396525792, 10.197362491062709, 8.959369148526873, 8.111882039478846, 7.3583952358790805,\n",
        "    6.742632559367588, 6.634049892425537, 6.143478325435093, 5.807376555034092, 5.844632625579834,\n",
        "    5.591985532215664, 5.314040047781808, 5.006323678152902, 4.983042461531503, 4.797314030783517,\n",
        "    4.790436404091971, 4.66345865385873, 4.589493479047503, 4.615839770862034, 4.376040152141026,\n",
        "    4.392090371676853, 4.267454079219273, 4.071239760943821, 4.083013789994376, 4.023034266063145,\n",
        "    3.893342205456325, 3.961550303867885, 3.8238401242664883, 3.8260814802987233, 3.768895983695984,\n",
        "    3.6374368837901523, 3.667690736906869, 3.5331750086375644, 3.5213640247072493, 3.532508747918265,\n",
        "    3.529361571584429, 3.4696840729032243, 3.4282524585723877, 3.392194083758763, 3.450771995953151,\n",
        "    3.315987161227635, 3.582858715738569, 3.472845230783735, 3.341963768005371, 3.2398096663611278,\n",
        "    3.1947281701224193, 3.262454526765006, 3.201199327196394, 3.163353749683925, 3.1110603979655673,\n",
        "    3.113699878965105, 3.089980517114912, 2.9907722813742503, 3.081830246107919, 2.9563471930367604,\n",
        "    2.9997369050979614, 3.0037802968706404, 3.0078927108219693, 3.086635947227478, 2.958267160824367,\n",
        "    2.932515467916216, 2.9392327070236206, 2.8950248786381314, 2.80914763041905, 2.939070531300136,\n",
        "    2.8827366828918457, 2.906066690172468, 2.78228611605508, 2.7861547470092773, 2.798818366868155,\n",
        "    2.7751307487487793, 2.747038551739284, 2.7546065534864153, 2.7774675403322493, 2.77508544921875,\n",
        "    2.773702996117728, 2.7331090484346663, 2.8323803458895003, 2.7189941235951016, 2.7103371790477206,\n",
        "    2.7096786499023438, 2.677763206618173, 2.6101122753960744, 2.6476376397269115, 2.6617040123258318,\n",
        "    2.575154049055917, 2.6234340838023593, 2.530229023524693, 2.687024814741952, 2.581781966345651,\n",
        "    2.6392320905412947, 2.64791282585689, 2.7034865617752075, 2.574937547956194, 2.585672233785902,\n",
        "    2.6209466797964915, 2.646137765475682, 2.5753627845219205, 2.725035343851362, 2.6867771659578596,\n",
        "    2.567517246518816, 2.4998758350099837, 2.4633253812789917, 2.5374621834073747, 2.4711793831416538,\n",
        "    2.478224890572684, 2.46951230934688, 2.458404907158443, 2.4824340513774326, 2.473321693284171,\n",
        "    2.398397045476096, 2.46338905606951, 2.4625627994537354, 2.390879137175424, 2.4836734192711964,\n",
        "    2.5056286369051253, 2.5320194278444563, 2.4751067672457014, 2.4593448809215, 2.4281612804957797,\n",
        "    2.469981244632176, 2.3584043043000356, 2.4079691086496626, 2.4393217393330167, 2.462585849421365,\n",
        "    2.477842228753226, 2.5042186890329634, 2.5271664432116916, 2.619382245200021, 2.637970498629979,\n",
        "    2.5448556457247054, 2.4718018770217896, 2.4763576643807546, 2.434452874319894, 2.3880372047424316,\n",
        "    2.3996316705431258, 2.416580183165414, 2.352173089981079, 2.370722327913557, 2.3763825382505144\n",
        "]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, loss_values, marker='o', linestyle='-')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGrVbx5nAYvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}